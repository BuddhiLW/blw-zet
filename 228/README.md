# My adventure into Infrastructure Land

The *end state* of my journey was: a `Kubernetes` two-nodes baremetal both running a `TalOS` setup (1 worker node; 1 control-panel node). This was *the best* trade-off for setting up baremetal servers for my kubernetes cluster.

First, I tried the `Ubuntu Server` route. Because, I have used `Linux` for almost a decade now. I know how to live in a terminal. So, my reasoning was: **how hard** would it be to setup a kubernetes cluster that has that "just works" quality and effort? Pretty effortless, I supposed.

So, I dived into **The Kubernetes Book by Nigel Poulton**. At the end of the month, I had worked it through. Kubernetes is a beautiful abstraction! But, we can't lie that it is, in itself, complex. As in, the task it solves structurally has a complex nature. And, Kubernetes does it's best to minimize it. Its patterns being declarative; reproducible; and, modular. We declare *what state we want* our cluster to be in; and, hopefully, kubernetes does it's thing, updating states, processes, images etc, until we get it.

It's an *agnostic* way to run and manage processes, which are running `containerized images`. *Without* thinking about the underlying infrastructure it's running on.

**Alas**, reality kicks-in, and we start to see the unfolding discrepancies between the good-world of the abstraction and the bad-world of real-systems running on an infrastructure, with its idiosyncrasies.

Now, I new, based on the sequel, I would need to face the ugly parts! A double-NAT (CGNAT) with two bare-metal servers would not be tamed by the light-hearted.

The CNI (Control Network Interface) - that lives in Layer 2 of the OSI model - was crumbling, never mind all the setups and `yaml` files I would write to make kubernetes behave properly. `Cilium` didn't solve it; tailscale end-to-end didn't solve it; nothing could stop it.

Then, I had to face it: there was no easy-path towards a happy ending. 

At these moments, we have to (re)consider. And, the path of least resistance, not rarely, resides in taking two-steps back, in order to take one-step forward. **Interdum retrocedere est progredi**.

So, I looked into more kubernetes-centric solutions. `Red Hat` seemed interesting; but, too specific to it's environment. And, I didn't wish to fall into the same issue I had with Ubuntu, to have to master yet another platform and ecosystem. When all I wanted was to enable computer-A to talk to computer-B reliably.

The most vanilla solution, yet very strange looking, was: **TalOS**! Why not? I'm not tied to a environment or platform. Even though it may not be particularly mainstream... It has all the qualities that matched kubernetes. Declarative, modular, and kubernetes-focused.

All those years installing linux OSes, setting up (linux desktop) machines, etc made me fearless of plugging in a pendrive and booting on a simple linux distro made for an specific use-case.

The down side? You can't just *ssh* into a standard-looking server. There is no entry-point into an imperative system! That may look like a down-side. But, that comes with the declarative, reproducible, promise! You shouldn't be able to drift the state of your machine. You must declare and apply it.

So, how do you talk to your server? Through applying `yaml` configs, through `talosctl`, similar to how you would use `kubectl`! 

Perfect. Plug your notebook to the modem through `ethernet`, and just hack it away. Then, apply a tailscale add-on; so, you can command your TalOS machines from anywhere. 

My experience was facing very clear tutorials and with that "just-works" quality in its documentation! It may be scary, because it was a new tool, a new way to manage a server, but, it was definitely worth it.

It was yet another abstraction, but it promised to eliminate any bare-metal idiosyncratic hard-edges. And, it did!

After installing `Cilium` and a `MetalLb`, I never had to touch it again, after that. And, I can easily see what's happening in the underlying machines through `talosctl dashboard`. Although, everything glued.

Thanks to the kubernetes way, I reused all the yaml configs I had previously, up to that point, to spin up `ArgoCD`, my apps, `Cloudflare` (in order to proxy through my CGNAT and expose the services to the internet), `Envoy Gateway` etc.

After not much effort further, I had a professional kubernetes cluster running everything. 100% control, no vendor lock in; no insane bills.

> Sapientia sola res est, cuius pretium ipsum est.
> Seneca, Epistulae Morales ad Lucilium


